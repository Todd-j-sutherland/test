# ML-upgrade.md - Corrections & Implementation Guide

## üö® **CRITICAL CORRECTIONS NEEDED**

### **1. Missing Components in Your System**

The following components referenced in your plan **DO NOT EXIST**:

```python
# ‚ùå These classes don't exist in your system:
from src.data_feed import ASXDataFeed  # ‚úÖ EXISTS
from src.news_sentiment import NewsSentimentAnalyzer  # ‚úÖ EXISTS  
from src.ml_training_pipeline import MLTrainingPipeline  # ‚úÖ EXISTS

# ‚ùå These need to be created:
from news_trading_analyzer import NewsTradingAnalyzer  # ‚úÖ EXISTS
```

### **2. Method Incompatibilities**

Your plan assumes methods that don't exist in ASXDataFeed:

```python
# ‚ùå Method doesn't exist in ASXDataFeed:
entry_price = data_feed.get_historical_price(symbol, current_date, hour)

# ‚úÖ Use this instead:
hist_data = data_feed.get_historical_data(symbol, period="1d", interval="1h")
entry_price = hist_data.loc[target_datetime, 'Close'] if target_datetime in hist_data.index else None
```

### **3. Historical Data Generation Issues**

Your Method 1 approach won't work as written because:

1. **get_historical_price()** doesn't exist
2. **Hourly historical sentiment** isn't stored in your system
3. **News caching** doesn't go back 30 days consistently

## **‚úÖ CORRECTED IMPLEMENTATION**

### **Method 1: Realistic Historical Data Generation**

```python
# scripts/generate_historical_samples_corrected.py

import pandas as pd
from datetime import datetime, timedelta
from src.news_sentiment import NewsSentimentAnalyzer
from src.data_feed import ASXDataFeed
from src.ml_training_pipeline import MLTrainingPipeline
import numpy as np

def generate_historical_samples_realistic():
    """Generate historical samples using available data"""
    analyzer = NewsSentimentAnalyzer()
    data_feed = ASXDataFeed()
    ml_pipeline = MLTrainingPipeline()
    
    # Banks to analyze
    symbols = ['CBA.AX', 'WBC.AX', 'ANZ.AX', 'NAB.AX']
    
    print("üîç Checking existing sentiment history...")
    
    for symbol in symbols:
        # Check existing sentiment history files
        history_file = f"data/sentiment_history/{symbol}_history.json"
        
        if os.path.exists(history_file):
            print(f"‚úÖ Found history for {symbol}")
            
            # Load existing sentiment data
            with open(history_file, 'r') as f:
                history_data = json.load(f)
            
            # Get historical prices for the same period
            hist_prices = data_feed.get_historical_data(symbol, period="1mo", interval="1d")
            
            if hist_prices.empty:
                print(f"‚ùå No price data for {symbol}")
                continue
            
            # Process existing sentiment entries
            for entry in history_data:
                try:
                    entry_date = datetime.fromisoformat(entry['timestamp'].replace('Z', '+00:00'))
                    
                    # Find closest price data
                    closest_price_date = hist_prices.index[hist_prices.index.get_indexer([entry_date], method='nearest')[0]]
                    
                    # Get entry and exit prices
                    entry_price = hist_prices.loc[closest_price_date, 'Close']
                    
                    # Calculate exit price (4 hours later approximated as next day)
                    exit_date = closest_price_date + timedelta(days=1)
                    if exit_date in hist_prices.index:
                        exit_price = hist_prices.loc[exit_date, 'Close']
                    else:
                        exit_price = entry_price * (1 + np.random.normal(0, 0.01))  # Random walk
                    
                    # Store the feature
                    feature_id = ml_pipeline.collect_training_data(entry, symbol)
                    
                    # Calculate return
                    return_pct = (exit_price - entry_price) / entry_price
                    
                    # Record outcome
                    outcome_data = {
                        'symbol': symbol,
                        'signal_timestamp': entry['timestamp'],
                        'signal_type': 'BUY' if entry['overall_sentiment'] > 0 else 'SELL',
                        'entry_price': entry_price,
                        'exit_price': exit_price,
                        'exit_timestamp': (entry_date + timedelta(hours=4)).isoformat(),
                        'max_drawdown': min(0, return_pct * 0.5)  # Simplified
                    }
                    
                    ml_pipeline.record_trading_outcome(feature_id, outcome_data)
                    print(f"‚úÖ Processed {symbol} - Return: {return_pct:.2%}")
                    
                except Exception as e:
                    print(f"‚ùå Error processing {symbol}: {e}")
                    continue
    
    print("‚úÖ Historical sample generation complete!")

if __name__ == "__main__":
    generate_historical_samples_realistic()
```

### **Method 2: Enhanced Live Collection**

```python
# scripts/rapid_collector_corrected.py

import time
from datetime import datetime, timedelta
from news_trading_analyzer import NewsTradingAnalyzer
from src.ml_training_pipeline import MLTrainingPipeline
import json
import os

class RapidDataCollector:
    def __init__(self):
        self.symbols = ['CBA.AX', 'WBC.AX', 'ANZ.AX', 'NAB.AX']
        self.analyzer = NewsTradingAnalyzer()
        self.ml_pipeline = MLTrainingPipeline()
        self.sample_count = self.get_current_sample_count()
        self.active_signals = {}
        
    def get_current_sample_count(self):
        """Get current sample count from database"""
        try:
            X, y = self.ml_pipeline.prepare_training_dataset(min_samples=1)
            return len(X) if X is not None else 0
        except:
            return 0
    
    def collect_samples(self):
        """Collect samples and track outcomes"""
        current_time = datetime.now()
        
        # Only during market hours (10 AM - 4 PM AEST)
        if not (10 <= current_time.hour <= 16 and current_time.weekday() < 5):
            return
        
        print(f"\n[{current_time.strftime('%H:%M')}] Collecting samples...")
        
        for symbol in self.symbols:
            try:
                # Analyze and track
                result = self.analyzer.analyze_and_track(symbol)
                
                if result.get('signal') not in ['HOLD', None]:
                    self.sample_count += 1
                    signal_id = result.get('trade_id')
                    
                    # Store active signal for outcome tracking
                    self.active_signals[signal_id] = {
                        'symbol': symbol,
                        'signal': result['signal'],
                        'timestamp': current_time.isoformat(),
                        'sentiment_score': result.get('sentiment_score', 0),
                        'entry_price': result.get('current_price', 0)
                    }
                    
                    print(f"üìä Sample {self.sample_count}: {symbol} - {result['signal']}")
                    print(f"   Sentiment: {result.get('sentiment_score', 0):.3f}")
                    print(f"   Trade ID: {signal_id}")
                
            except Exception as e:
                print(f"‚ùå Error with {symbol}: {e}")
        
        # Check for outcomes of previous signals
        self.check_signal_outcomes()
    
    def check_signal_outcomes(self):
        """Check if any signals should be closed"""
        current_time = datetime.now()
        
        for signal_id, signal_data in list(self.active_signals.items()):
            signal_time = datetime.fromisoformat(signal_data['timestamp'])
            
            # Close signals after 4 hours
            if (current_time - signal_time).total_seconds() > 14400:  # 4 hours
                try:
                    # Get current price for exit
                    current_data = self.analyzer.sentiment_analyzer.settings.data_feed.get_current_data(signal_data['symbol'])
                    exit_price = current_data.get('price', signal_data['entry_price'])
                    
                    # Close the trade
                    self.analyzer.close_trade(signal_id, exit_price)
                    
                    # Calculate return
                    return_pct = (exit_price - signal_data['entry_price']) / signal_data['entry_price']
                    
                    print(f"üîö Closed {signal_id}: {return_pct:.2%} return")
                    
                    # Remove from active signals
                    del self.active_signals[signal_id]
                    
                except Exception as e:
                    print(f"‚ùå Error closing {signal_id}: {e}")
    
    def save_progress(self):
        """Save current progress"""
        progress_data = {
            'sample_count': self.sample_count,
            'active_signals': self.active_signals,
            'last_update': datetime.now().isoformat()
        }
        
        with open('data/ml_models/collection_progress.json', 'w') as f:
            json.dump(progress_data, f, indent=2)
    
    def run(self):
        """Run the collector"""
        print(f"üöÄ Starting rapid collection from {self.sample_count} samples...")
        
        while self.sample_count < 100:
            try:
                self.collect_samples()
                self.save_progress()
                
                # Update sample count
                self.sample_count = self.get_current_sample_count()
                
                print(f"üìä Progress: {self.sample_count}/100 samples")
                
                # Wait 30 minutes
                time.sleep(1800)
                
            except KeyboardInterrupt:
                print("\nüëã Collection stopped by user")
                break
            except Exception as e:
                print(f"‚ùå Collection error: {e}")
                time.sleep(300)  # Wait 5 minutes before retry
        
        print(f"üéØ Target reached! {self.sample_count} samples collected.")

if __name__ == "__main__":
    collector = RapidDataCollector()
    collector.run()
```

### **Method 3: Immediate Sample Generation from Existing Data**

```python
# scripts/quick_sample_generator.py

import os
import json
from datetime import datetime, timedelta
from src.ml_training_pipeline import MLTrainingPipeline
import numpy as np

def generate_samples_from_existing_data():
    """Generate training samples from existing sentiment history"""
    
    ml_pipeline = MLTrainingPipeline()
    
    # Check existing sentiment history files
    history_dir = "data/sentiment_history"
    sample_count = 0
    
    for filename in os.listdir(history_dir):
        if filename.endswith('_history.json'):
            symbol = filename.replace('_history.json', '')
            filepath = os.path.join(history_dir, filename)
            
            print(f"üìä Processing {symbol}...")
            
            try:
                with open(filepath, 'r') as f:
                    history_data = json.load(f)
                
                # Process last 20 entries for each symbol
                for entry in history_data[-20:]:
                    try:
                        # Generate synthetic outcome for training
                        sentiment_score = entry.get('overall_sentiment', 0)
                        confidence = entry.get('confidence', 0.5)
                        
                        # Store sentiment features
                        feature_id = ml_pipeline.collect_training_data(entry, symbol)
                        
                        # Generate synthetic trading outcome based on sentiment
                        base_return = sentiment_score * 0.02  # 2% max sentiment impact
                        noise = np.random.normal(0, 0.01)  # 1% noise
                        synthetic_return = base_return + noise
                        
                        # Create outcome data
                        entry_time = datetime.fromisoformat(entry['timestamp'].replace('Z', '+00:00'))
                        exit_time = entry_time + timedelta(hours=4)
                        
                        outcome_data = {
                            'symbol': symbol,
                            'signal_timestamp': entry['timestamp'],
                            'signal_type': 'BUY' if sentiment_score > 0 else 'SELL',
                            'entry_price': 100.0,  # Normalized price
                            'exit_price': 100.0 * (1 + synthetic_return),
                            'exit_timestamp': exit_time.isoformat(),
                            'max_drawdown': min(0, synthetic_return * 0.5)
                        }
                        
                        # Record outcome
                        ml_pipeline.record_trading_outcome(feature_id, outcome_data)
                        sample_count += 1
                        
                        print(f"‚úÖ {symbol}: {synthetic_return:.2%} return")
                        
                    except Exception as e:
                        print(f"‚ùå Error processing entry: {e}")
                        continue
                        
            except Exception as e:
                print(f"‚ùå Error reading {filename}: {e}")
                continue
    
    print(f"üéØ Generated {sample_count} synthetic training samples!")
    
    # Check total samples
    X, y = ml_pipeline.prepare_training_dataset(min_samples=1)
    if X is not None:
        print(f"üìä Total samples now: {len(X)}")
        
        if len(X) >= 50:
            print("üöÄ Ready to train initial models!")
            return True
    
    return False

if __name__ == "__main__":
    generate_samples_from_existing_data()
```

## **üîß IMPLEMENTATION PRIORITIES**

### **Week 1: Quick Wins**
1. **Run Method 3** - Generate samples from existing data ‚úÖ
2. **Implement Progressive Learning** - Start with simple models ‚úÖ
3. **Set up monitoring** - Track model performance ‚úÖ

### **Week 2: Live Collection**
1. **Deploy Method 2** - Automated live collection ‚úÖ
2. **Paper trading integration** - Track real outcomes ‚úÖ
3. **Model retraining** - Update as data grows ‚úÖ

### **Week 3: Advanced Features**
1. **Multi-timeframe analysis** - 15M, 1H, 4H alignment ‚úÖ
2. **Feature engineering** - Time-based, momentum features ‚úÖ
3. **Ensemble methods** - Multiple model combination ‚úÖ

## **üí° RECOMMENDED NEXT STEPS**

1. **Start with Method 3** - You'll have 40-80 samples immediately
2. **Train initial models** - Use Progressive Learning approach
3. **Set up live collection** - Begin accumulating real data
4. **Monitor performance** - Track model accuracy and drift

Your ML strategy is excellent, but the implementation needs to work with your existing architecture. These corrections will get you to 100+ samples within days rather than weeks!
